{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI0xl50unLYT"
      },
      "outputs": [],
      "source": [
        "# Installing necessary libraries\n",
        "!pip install unsloth vllm                # Install Unsloth and vLLM (for fast inference/training)\n",
        "!pip install --upgrade pillow            # Upgrade PIL (used for image processing)\n",
        "!pip install datasets                    # Hugging Face datasets library\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b  # Install specific TRL commit (GRPO support)\n",
        "!pip install tensorboard                 # TensorBoard for visualizing training logs\n",
        "\n",
        "# Clean up existing PIL or Google modules from memory (for fresh imports later)\n",
        "import sys\n",
        "modules = list(sys.modules.keys())\n",
        "for x in modules:\n",
        "    if \"PIL\" in x or \"google\" in x:\n",
        "        sys.modules.pop(x)               # Unload any conflicting or partially loaded modules\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import from Unsloth: FastLanguageModel is a wrapper to simplify loading/training LLMs\n",
        "# PatchFastRL patches the model to support GRPO (a reinforcement learning algorithm)\n",
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "import torch\n",
        "\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)  # Patch the FastLanguageModel with GRPO functionality\n",
        "\n",
        "# Configuration parameters\n",
        "max_seq_length = 1024       # Maximum length of a sequence during training\n",
        "lora_rank = 64              # Rank used in LoRA (Low-Rank Adaptation)\n",
        "\n",
        "# Load the model using Unsloth's FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"google/gemma-2-2b-it\",  # Name of the base model (Gemma 2B Instruction-tuned)\n",
        "    max_seq_length=max_seq_length,      # Pass max sequence length\n",
        "    load_in_4bit=True,                  # Load model in 4-bit precision (saves memory)\n",
        "    fast_inference=False,              # Disable inference optimizations (can set to True for inference-only mode) for (reduce latency, minimize computational costs, and improve scalability)\n",
        "    max_lora_rank=lora_rank,           # Set LoRA rank for Unsloth internal validation\n",
        "    gpu_memory_utilization=0.5,        # Limit GPU memory use to 50%\n",
        ")\n"
      ],
      "metadata": {
        "id": "wH2isxfUnWGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA (parameter-efficient fine-tuning) to the model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,                        # LoRA rank\n",
        "    target_modules=[                   # List of model modules where LoRA will be applied\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_rank,              # Scaling factor for LoRA\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Enable gradient checkpointing to save memory\n",
        "    random_state=3407,                 # Set a fixed seed for reproducibility\n",
        ")\n"
      ],
      "metadata": {
        "id": "CjGz_e94oKzk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the PokerBench dataset.\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "import re\n",
        "\n",
        "# System instruction to be included in the user prompt\n",
        "SYSTEM_INSTRUCTION = \"\"\"You are an expert poker player. Your response must be in the format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\"\"\"\n",
        "\n",
        "def inspect_dataset():\n",
        "    \"\"\"Inspect the PokerBench dataset to understand its structure.\"\"\"\n",
        "    data = load_dataset(\"RZ412/PokerBench\")[\"train\"]\n",
        "    sample = data[0]\n",
        "    print(\"Sample PokerBench record:\", sample)\n",
        "    print(\"Available keys:\", list(sample.keys()))\n",
        "    return data\n",
        "\n",
        "def format_poker_prompt(game_state: dict) -> list:\n",
        "    \"\"\"Construct zero-shot prompt from game state, compatible with Gemma-2-2B.\"\"\"\n",
        "    # Fallback values for missing keys\n",
        "    position = game_state.get('position', 'Unknown')\n",
        "    stack = game_state.get('stack', 'Unknown')\n",
        "    hand = game_state.get('hand', 'Unknown')\n",
        "    community_cards = game_state.get('community_cards', 'None')\n",
        "    pot = game_state.get('pot', 'Unknown')\n",
        "    to_call = game_state.get('to_call', '0')\n",
        "    opponent_actions = game_state.get('opponent_actions', 'None')\n",
        "\n",
        "    # Check for alternative field names (adjust based on inspection)\n",
        "    if 'player_position' in game_state:\n",
        "        position = game_state['player_position']\n",
        "    if 'stack_size' in game_state:\n",
        "        stack = game_state['stack_size']\n",
        "    if 'cards' in game_state:\n",
        "        hand = game_state['cards']\n",
        "    if 'board' in game_state:\n",
        "        community_cards = game_state['board']\n",
        "    if 'pot_size' in game_state:\n",
        "        pot = game_state['pot_size']\n",
        "    if 'amount_to_call' in game_state:\n",
        "        to_call = game_state['amount_to_call']\n",
        "    if 'actions' in game_state:\n",
        "        opponent_actions = game_state['actions']\n",
        "\n",
        "    prompt = f\"\"\"{SYSTEM_INSTRUCTION}\n",
        "\n",
        "Game State:\n",
        "- Position: {position}\n",
        "- Stack: {stack}\n",
        "- Hand: {hand}\n",
        "- Community Cards: {community_cards}\n",
        "- Pot: {pot}\n",
        "- To Call: {to_call}\n",
        "- Opponent Actions: {opponent_actions}\n",
        "\n",
        "What action should you take (fold, call, raise, check)? Provide reasoning and final action.\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"Extract answer from XML-formatted response.\"\"\"\n",
        "    try:\n",
        "        answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
        "        return answer\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def get_pokerbench_dataset(split=\"train\") -> Dataset:\n",
        "    \"\"\"Load and preprocess PokerBench dataset.\"\"\"\n",
        "    data = load_dataset(\"RZ412/PokerBench\")[split]\n",
        "\n",
        "    # Inspect the dataset to confirm structure\n",
        "    print(\"Inspecting PokerBench dataset...\")\n",
        "    sample = data[0]\n",
        "    print(\"Sample record:\", sample)\n",
        "    print(\"Available keys:\", list(sample.keys()))\n",
        "\n",
        "    # Test the chat template with a sample prompt\n",
        "    print(\"Testing chat template...\")\n",
        "    sample_prompt = format_poker_prompt(sample)\n",
        "    try:\n",
        "        test_output = tokenizer.apply_chat_template(sample_prompt, tokenize=False, add_generation_prompt=True)\n",
        "        print(\"Sample formatted prompt:\", test_output)\n",
        "    except Exception as e:\n",
        "        print(f\"Chat template error: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Map the dataset to create prompts\n",
        "    try:\n",
        "        data = data.map(\n",
        "            lambda x: {\n",
        "                \"prompt\": format_poker_prompt(x),\n",
        "                \"answer\": x.get(\"action\", \"\")  # Use .get to avoid KeyError\n",
        "            },\n",
        "            batched=False  # Process one example at a time to isolate errors\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during dataset mapping: {e}\")\n",
        "        raise\n",
        "    return data\n",
        "\n",
        "# Load and preprocess dataset\n",
        "dataset = get_pokerbench_dataset()"
      ],
      "metadata": {
        "id": "17jHswhoOUAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfaed78-236a-4dec-a0ce-26d9a7c8d74d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting PokerBench dataset...\n",
            "Sample record: {'instruction': '\\n\\nYou are a specialist in playing 6-handed No Limit Texas Holdem. The following will be a game scenario and you need to make the optimal decision.\\n\\nHere is a game summary:\\n\\nThe small blind is 0.5 chips and the big blind is 1 chips. Everyone started with 100 chips.\\nThe player positions involved in this game are UTG, HJ, CO, BTN, SB, BB.\\nIn this hand, your position is HJ, and your holding is [King of Diamond and Jack of Spade].\\nBefore the flop, HJ raise 2.0 chips, and BB call. Assume that all other players that is not mentioned folded.\\nThe flop comes King Of Spade, Seven Of Heart, and Two Of Diamond, then BB check, and HJ check.\\nThe turn comes Jack Of Club, then BB check, HJ bet 3 chips, BB raise 10 chips, and HJ call.\\nThe river comes Seven Of Club, then BB check.\\n\\n\\nNow it is your turn to make a move.\\nTo remind you, the current pot size is 24.0 chips, and your holding is [King of Diamond and Jack of Spade].\\n\\nDecide on an action based on the strength of your hand on this board, your position, and actions before you. Do not explain your answer.\\nYour optimal action is:', 'output': 'bet 18'}\n",
            "Available keys: ['instruction', 'output']\n",
            "Testing chat template...\n",
            "Sample formatted prompt: <bos><start_of_turn>user\n",
            "You are an expert poker player. Your response must be in the format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer>\n",
            "\n",
            "Game State:\n",
            "- Position: Unknown\n",
            "- Stack: Unknown\n",
            "- Hand: Unknown\n",
            "- Community Cards: None\n",
            "- Pot: Unknown\n",
            "- To Call: 0\n",
            "- Opponent Actions: None\n",
            "\n",
            "What action should you take (fold, call, raise, check)? Provide reasoning and final action.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward Functions\n",
        "# Define reward functions for GRPO training.\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    '''Checks if the model gave the correct final answer, compared to the ground-truth answer'''\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted = [extract_xml_answer(r) for r in responses]\n",
        "    return [2.0 if r.lower() == a.lower() else 0.0 for r, a in zip(extracted, answer)]\n",
        "\n",
        "def valid_action_reward_func(completions, **kwargs) -> list[float]:\n",
        "  '''Checks whether the model generated a valid poker action, even if it's not the correct one'''\n",
        "    valid_actions = {\"fold\", \"call\", \"raise\", \"check\"}\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted = [extract_xml_answer(r).lower() for r in responses]\n",
        "    return [0.5 if r in valid_actions else 0.0 for r in extracted]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "  '''Enforces that the model strictly follows the expected response format'''\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [0.5 if re.match(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "  '''-More lenient formatting check than the strict regex version\n",
        "     - Measures partial compliance with the expected tags '''\n",
        "    def count_xml(text: str) -> float:\n",
        "        count = 0.0\n",
        "        if text.count(\"<reasoning>\\n\") == 1:\n",
        "            count += 0.125\n",
        "        if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "            count += 0.125\n",
        "        if text.count(\"\\n<answer>\\n\") == 1:\n",
        "            count += 0.125\n",
        "        if text.count(\"\\n</answer>\") == 1:\n",
        "            count += 0.125\n",
        "        return count\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(r) for r in responses]"
      ],
      "metadata": {
        "id": "D2r9vewwoOVQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with GRPO\n",
        "# Configure and run GRPO training.\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from unsloth import is_bfloat16_supported   # Utility from Unsloth to detect hardware support for BF16\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm=False,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=4,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    max_steps=50,\n",
        "    save_steps=50,\n",
        "    max_grad_norm=0.1,   # Clip gradients to avoid exploding gradients\n",
        "    report_to=\"tensorboard\",\n",
        "    output_dir=\"outputs\",\n",
        "    run_name=\"gemma_pokerbench\",\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        valid_action_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NyaZHuJIpo3H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c4ddc6d-808f-4077-b3ee-0b09e5cff451"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 563,200 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 83,066,880 of 2,697,408,768 (3.08% trained)\n",
            "/usr/local/lib/python3.11/dist-packages/unsloth/kernels/utils.py:665: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 2304], which does not match the required output shape [16, 1, 2304]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
            "  out = torch_matmul(X, W, out = out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 22:38, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / xmlcount_reward_func</th>\n",
              "      <th>rewards / strict_format_reward_func</th>\n",
              "      <th>rewards / valid_action_reward_func</th>\n",
              "      <th>rewards / correctness_reward_func</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.285264</td>\n",
              "      <td>173.125000</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.375411</td>\n",
              "      <td>175.750000</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.213766</td>\n",
              "      <td>159.375000</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.320312</td>\n",
              "      <td>0.206897</td>\n",
              "      <td>179.375000</td>\n",
              "      <td>0.003973</td>\n",
              "      <td>0.289062</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.398438</td>\n",
              "      <td>0.230540</td>\n",
              "      <td>181.062500</td>\n",
              "      <td>0.035626</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.184912</td>\n",
              "      <td>165.875000</td>\n",
              "      <td>0.137117</td>\n",
              "      <td>0.343750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.304688</td>\n",
              "      <td>0.222113</td>\n",
              "      <td>184.437500</td>\n",
              "      <td>0.271274</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.396465</td>\n",
              "      <td>186.937500</td>\n",
              "      <td>0.509281</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.242188</td>\n",
              "      <td>0.110462</td>\n",
              "      <td>195.187500</td>\n",
              "      <td>0.761819</td>\n",
              "      <td>0.242188</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.094837</td>\n",
              "      <td>189.125000</td>\n",
              "      <td>1.745564</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.134669</td>\n",
              "      <td>195.375000</td>\n",
              "      <td>4.281929</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>0.226562</td>\n",
              "      <td>0.157767</td>\n",
              "      <td>190.812500</td>\n",
              "      <td>3.608238</td>\n",
              "      <td>0.226562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.155986</td>\n",
              "      <td>184.750000</td>\n",
              "      <td>6.253275</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.420091</td>\n",
              "      <td>177.625000</td>\n",
              "      <td>9.033752</td>\n",
              "      <td>0.281250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.012300</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.346188</td>\n",
              "      <td>176.562500</td>\n",
              "      <td>12.332429</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.019900</td>\n",
              "      <td>0.226562</td>\n",
              "      <td>0.124677</td>\n",
              "      <td>194.375000</td>\n",
              "      <td>19.909624</td>\n",
              "      <td>0.226562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.140015</td>\n",
              "      <td>193.500000</td>\n",
              "      <td>9.042400</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.304688</td>\n",
              "      <td>0.419988</td>\n",
              "      <td>181.000000</td>\n",
              "      <td>11.860590</td>\n",
              "      <td>0.179688</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>0.382812</td>\n",
              "      <td>0.402688</td>\n",
              "      <td>175.937500</td>\n",
              "      <td>9.036067</td>\n",
              "      <td>0.226562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.007400</td>\n",
              "      <td>0.343750</td>\n",
              "      <td>0.377248</td>\n",
              "      <td>181.312500</td>\n",
              "      <td>7.398690</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.008200</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>0.238885</td>\n",
              "      <td>176.062500</td>\n",
              "      <td>8.231931</td>\n",
              "      <td>0.242188</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.029600</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>192.812500</td>\n",
              "      <td>29.610184</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.163873</td>\n",
              "      <td>168.750000</td>\n",
              "      <td>10.448026</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>0.341510</td>\n",
              "      <td>193.062500</td>\n",
              "      <td>14.550738</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.040600</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.344837</td>\n",
              "      <td>198.187500</td>\n",
              "      <td>40.640274</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.031100</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>0.152905</td>\n",
              "      <td>189.125000</td>\n",
              "      <td>31.084913</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.022400</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>192.562500</td>\n",
              "      <td>22.426325</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.016100</td>\n",
              "      <td>0.101562</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>184.812500</td>\n",
              "      <td>16.129799</td>\n",
              "      <td>0.101562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.022900</td>\n",
              "      <td>0.117188</td>\n",
              "      <td>0.141920</td>\n",
              "      <td>194.125000</td>\n",
              "      <td>22.905973</td>\n",
              "      <td>0.117188</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.019700</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>184.312500</td>\n",
              "      <td>19.674999</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.021500</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.267409</td>\n",
              "      <td>185.125000</td>\n",
              "      <td>21.490093</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.022000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>191.375000</td>\n",
              "      <td>21.976309</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.020900</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>187.062500</td>\n",
              "      <td>20.864599</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.016100</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>0.082959</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>16.061411</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.032000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>184.062500</td>\n",
              "      <td>32.020393</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.021500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.111792</td>\n",
              "      <td>183.187500</td>\n",
              "      <td>21.461739</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.139200</td>\n",
              "      <td>0.101562</td>\n",
              "      <td>0.148894</td>\n",
              "      <td>177.625000</td>\n",
              "      <td>139.201294</td>\n",
              "      <td>0.101562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.016600</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.029920</td>\n",
              "      <td>176.625000</td>\n",
              "      <td>16.564848</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.018300</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.063587</td>\n",
              "      <td>185.125000</td>\n",
              "      <td>18.260141</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>178.312500</td>\n",
              "      <td>16.031157</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.015300</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>182.437500</td>\n",
              "      <td>15.253674</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.018100</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>0.122319</td>\n",
              "      <td>192.187500</td>\n",
              "      <td>18.062923</td>\n",
              "      <td>0.085938</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.022200</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>194.687500</td>\n",
              "      <td>22.183664</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.017500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>180.937500</td>\n",
              "      <td>17.456375</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.018000</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>188.312500</td>\n",
              "      <td>17.958050</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.018200</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>184.187500</td>\n",
              "      <td>18.191217</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>194.437500</td>\n",
              "      <td>21.432758</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.085377</td>\n",
              "      <td>184.437500</td>\n",
              "      <td>17.240496</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>178.937500</td>\n",
              "      <td>14.600785</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.018300</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.080542</td>\n",
              "      <td>194.250000</td>\n",
              "      <td>18.298851</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/unsloth/kernels/utils.py:665: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 2304], which does not match the required output shape [16, 1, 2304]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
            "  out = torch_matmul(X, W, out = out)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=0.016330718234474375, metrics={'train_runtime': 1397.1832, 'train_samples_per_second': 0.143, 'train_steps_per_second': 0.036, 'total_flos': 0.0, 'train_loss': 0.016330718234474375})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model.save_pretrained(\"/content/drive/MyDrive/lora_adapter\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKj8cma1TW6H",
        "outputId": "e39b65c4-cc1f-4071-8718-9ef8bb6a282e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from peft import PeftModel\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Sample test prompt (adjust keys based on dataset schema from inspection)\n",
        "test_prompt = format_poker_prompt({\n",
        "    \"player_position\": \"Button\",\n",
        "    \"stack_size\": 1000,\n",
        "    \"cards\": [\"As\", \"Kd\"],\n",
        "    \"board\": [],\n",
        "    \"pot_size\": 100,\n",
        "    \"amount_to_call\": 50,\n",
        "    \"actions\": [\"Small Blind posts 25\", \"Big Blind posts 50\"]\n",
        "})\n",
        "\n",
        "# Tokenize the prompt\n",
        "text = tokenizer.apply_chat_template(test_prompt, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(\"cuda\")\n",
        "\n",
        "# Debug input shape\n",
        "print(\"Input shape:\", inputs[\"input_ids\"].shape)\n",
        "\n",
        "# Inference without GRPO\n",
        "print(\"Inference without GRPO:\")\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvSxZla_ptFd",
        "outputId": "a62ab42c-7c60-4eec-eeeb-c381812cf906"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 140])\n",
            "Inference without GRPO:\n",
            "user\n",
            "You are an expert poker player. Your response must be in the format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer>\n",
            "\n",
            "Game State:\n",
            "- Position: Button\n",
            "- Stack: 1000\n",
            "- Hand: ['As', 'Kd']\n",
            "- Community Cards: []\n",
            "- Pot: 100\n",
            "- To Call: 50\n",
            "- Opponent Actions: ['Small Blind posts 25', 'Big Blind posts 50']\n",
            "\n",
            "What action should you take (fold, call, raise, check)? Provide reasoning and final action.\n",
            "model\n",
            "<reasoning> You have a decent pair of Aces and Kings, making it a strong hand.  However, the pot is still relatively small at 100. There's a good chance you'll face a larger bet on the flop, so calling might limit your range and lead to a potentially unfavorable showdown. </reasoning>\n",
            "<answer> **Raise** </answer> \n",
            " \n",
            "**Explanation:**\n",
            "\n",
            "* **Positional advantage:** Being in the button gives you the opportunity to see the flop with your pair before the other players have to act.\n",
            "* **Limiting the action:**  Raising forces your opponent to call with a strong hand and opens up the chance to steal the pot. \n",
            "* **Building the pot:** A raise sets a larger pot early on to minimize the pressure of betting into the flop\n",
            "* **Controlling the game:** This action can cause your opponent to fold or raise to try to get a better position in the hand. \n",
            "\n",
            "\n",
            "Let me know if you want to explore any further. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA\n",
        "model.save_pretrained(\"grpo_poker_lora\")\n",
        "# Inference with GRPO LoRA\n",
        "print(\"\\nInference with GRPO LoRA:\")\n",
        "# Load the LoRA weights onto the existing model\n",
        "model = PeftModel.from_pretrained(model, \"grpo_poker_lora\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "xMv9JH90psey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0fbb77-967e-4378-cffa-554ec136bb7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference with GRPO LoRA:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight'].\n",
            "  warnings.warn(warn_message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user\n",
            "You are an expert poker player. Your response must be in the format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer>\n",
            "\n",
            "Game State:\n",
            "- Position: Button\n",
            "- Stack: 1000\n",
            "- Hand: ['As', 'Kd']\n",
            "- Community Cards: []\n",
            "- Pot: 100\n",
            "- To Call: 50\n",
            "- Opponent Actions: ['Small Blind posts 25', 'Big Blind posts 50']\n",
            "\n",
            "What action should you take (fold, call, raise, check)? Provide reasoning and final action.\n",
            "model\n",
            "<reasoning>\n",
            "- **Position:** Button is a very strong position. You get to act last and have a good chance of controlling the pace of the hand.\n",
            "- **Hand:**  While Ace-King is a strong starting hand, it's not a dominant hand on its own. You have a strong top pair and an Ace, but you have no flush draws.\n",
            "- **Pot Odds:** The pot is 100. While not a huge pot, it's enough to make things interesting.\n",
            "- **Opponent Actions:**  The blinds are in, and the Big Blind is sizing up the pot.\n",
            "- **Opponent's Behavior:** The Big Blind's actions can be read as passive or aggressive depending on their playing style. It's difficult to know what the Big Blind is thinking. \n",
            "\n",
            "**Considering all these factors, I believe a small raise is the optimal play.**\n",
            "\n",
            "<answer>\n",
            "**Raise: 50**\n",
            "</answer> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ao38XjXrhfje"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}