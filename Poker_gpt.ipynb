{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI0xl50unLYT"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
        "!pip install tensorboard\n",
        "\n",
        "import sys\n",
        "modules = list(sys.modules.keys())\n",
        "for x in modules:\n",
        "    if \"PIL\" in x or \"google\" in x:\n",
        "        sys.modules.pop(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "import torch\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
        "\n",
        "max_seq_length = 1024\n",
        "lora_rank = 64\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"google/gemma-2-2b-it\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    fast_inference=False,\n",
        "    max_lora_rank=lora_rank,\n",
        "    gpu_memory_utilization=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "wH2isxfUnWGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_rank,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjGz_e94oKzk",
        "outputId": "26fc8d43-a92c-457c-95ea-cb472f83d25e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.8.1 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the PokerBench dataset.\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "import re\n",
        "\n",
        "# System instruction to be included in the user prompt\n",
        "SYSTEM_INSTRUCTION = \"\"\"You are an expert poker player. Your response must be in the format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\"\"\"\n",
        "\n",
        "def inspect_dataset():\n",
        "    \"\"\"Inspect the PokerBench dataset to understand its structure.\"\"\"\n",
        "    data = load_dataset(\"RZ412/PokerBench\")[\"train\"]\n",
        "    sample = data[0]\n",
        "    print(\"Sample PokerBench record:\", sample)\n",
        "    print(\"Available keys:\", list(sample.keys()))\n",
        "    return data\n",
        "\n",
        "def format_poker_prompt(game_state: dict) -> list:\n",
        "    \"\"\"Construct zero-shot prompt from game state, compatible with Gemma-2-2B.\"\"\"\n",
        "    # Fallback values for missing keys\n",
        "    position = game_state.get('position', 'Unknown')\n",
        "    stack = game_state.get('stack', 'Unknown')\n",
        "    hand = game_state.get('hand', 'Unknown')\n",
        "    community_cards = game_state.get('community_cards', 'None')\n",
        "    pot = game_state.get('pot', 'Unknown')\n",
        "    to_call = game_state.get('to_call', '0')\n",
        "    opponent_actions = game_state.get('opponent_actions', 'None')\n",
        "\n",
        "    # Check for alternative field names (adjust based on inspection)\n",
        "    if 'player_position' in game_state:\n",
        "        position = game_state['player_position']\n",
        "    if 'stack_size' in game_state:\n",
        "        stack = game_state['stack_size']\n",
        "    if 'cards' in game_state:\n",
        "        hand = game_state['cards']\n",
        "    if 'board' in game_state:\n",
        "        community_cards = game_state['board']\n",
        "    if 'pot_size' in game_state:\n",
        "        pot = game_state['pot_size']\n",
        "    if 'amount_to_call' in game_state:\n",
        "        to_call = game_state['amount_to_call']\n",
        "    if 'actions' in game_state:\n",
        "        opponent_actions = game_state['actions']\n",
        "\n",
        "    prompt = f\"\"\"{SYSTEM_INSTRUCTION}\n",
        "\n",
        "Game State:\n",
        "- Position: {position}\n",
        "- Stack: {stack}\n",
        "- Hand: {hand}\n",
        "- Community Cards: {community_cards}\n",
        "- Pot: {pot}\n",
        "- To Call: {to_call}\n",
        "- Opponent Actions: {opponent_actions}\n",
        "\n",
        "What action should you take (fold, call, raise, check)? Provide reasoning and final action.\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"Extract answer from XML-formatted response.\"\"\"\n",
        "    try:\n",
        "        answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
        "        return answer\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def get_pokerbench_dataset(split=\"train\") -> Dataset:\n",
        "    \"\"\"Load and preprocess PokerBench dataset.\"\"\"\n",
        "    data = load_dataset(\"RZ412/PokerBench\")[split]\n",
        "\n",
        "    # Inspect the dataset to confirm structure\n",
        "    print(\"Inspecting PokerBench dataset...\")\n",
        "    sample = data[0]\n",
        "    print(\"Sample record:\", sample)\n",
        "    print(\"Available keys:\", list(sample.keys()))\n",
        "\n",
        "    # Test the chat template with a sample prompt\n",
        "    print(\"Testing chat template...\")\n",
        "    sample_prompt = format_poker_prompt(sample)\n",
        "    try:\n",
        "        test_output = tokenizer.apply_chat_template(sample_prompt, tokenize=False, add_generation_prompt=True)\n",
        "        print(\"Sample formatted prompt:\", test_output)\n",
        "    except Exception as e:\n",
        "        print(f\"Chat template error: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Map the dataset to create prompts\n",
        "    try:\n",
        "        data = data.map(\n",
        "            lambda x: {\n",
        "                \"prompt\": format_poker_prompt(x),\n",
        "                \"answer\": x.get(\"action\", \"\")  # Use .get to avoid KeyError\n",
        "            },\n",
        "            batched=False  # Process one example at a time to isolate errors\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during dataset mapping: {e}\")\n",
        "        raise\n",
        "    return data\n",
        "\n",
        "# Load and preprocess dataset\n",
        "dataset = get_pokerbench_dataset()"
      ],
      "metadata": {
        "id": "17jHswhoOUAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Reward Functions\n",
        "# Define reward functions for GRPO training.\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted = [extract_xml_answer(r) for r in responses]\n",
        "    return [2.0 if r.lower() == a.lower() else 0.0 for r, a in zip(extracted, answer)]\n",
        "\n",
        "def valid_action_reward_func(completions, **kwargs) -> list[float]:\n",
        "    valid_actions = {\"fold\", \"call\", \"raise\", \"check\"}\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted = [extract_xml_answer(r).lower() for r in responses]\n",
        "    return [0.5 if r in valid_actions else 0.0 for r in extracted]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [0.5 if re.match(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    def count_xml(text: str) -> float:\n",
        "        count = 0.0\n",
        "        if text.count(\"<reasoning>\\n\") == 1:\n",
        "            count += 0.125\n",
        "        if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "            count += 0.125\n",
        "        if text.count(\"\\n<answer>\\n\") == 1:\n",
        "            count += 0.125\n",
        "        if text.count(\"\\n</answer>\") == 1:\n",
        "            count += 0.125\n",
        "        return count\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(r) for r in responses]"
      ],
      "metadata": {
        "id": "D2r9vewwoOVQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 5. Training with GRPO\n",
        "# Configure and run GRPO training.\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm=False,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=4,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    max_steps=1,\n",
        "    save_steps=1,\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"tensorboard\",\n",
        "    output_dir=\"outputs\",\n",
        "    run_name=\"gemma_pokerbench\",\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        valid_action_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NyaZHuJIpo3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model.save_pretrained(\"/content/drive/MyDrive/lora_adapter\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKj8cma1TW6H",
        "outputId": "becdec94-1aa1-4bd9-b978-13b80af1e8b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from peft import PeftModel\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Sample test prompt (adjust keys based on dataset schema from inspection)\n",
        "test_prompt = format_poker_prompt({\n",
        "    \"player_position\": \"Button\",\n",
        "    \"stack_size\": 1000,\n",
        "    \"cards\": [\"As\", \"Kd\"],\n",
        "    \"board\": [],\n",
        "    \"pot_size\": 100,\n",
        "    \"amount_to_call\": 50,\n",
        "    \"actions\": [\"Small Blind posts 25\", \"Big Blind posts 50\"]\n",
        "})\n",
        "\n",
        "# Tokenize the prompt\n",
        "text = tokenizer.apply_chat_template(test_prompt, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(\"cuda\")\n",
        "\n",
        "# Debug input shape\n",
        "print(\"Input shape:\", inputs[\"input_ids\"].shape)\n",
        "\n",
        "# Inference without GRPO\n",
        "print(\"Inference without GRPO:\")\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvSxZla_ptFd",
        "outputId": "a15b37fc-5f66-4599-9d88-4c22f7aad4d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 140])\n",
            "Inference without GRPO:\n",
            "user\n",
            "You are an expert poker player. Your response must be in the format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer>\n",
            "\n",
            "Game State:\n",
            "- Position: Button\n",
            "- Stack: 1000\n",
            "- Hand: ['As', 'Kd']\n",
            "- Community Cards: []\n",
            "- Pot: 100\n",
            "- To Call: 50\n",
            "- Opponent Actions: ['Small Blind posts 25', 'Big Blind posts 50']\n",
            "\n",
            "What action should you take (fold, call, raise, check)? Provide reasoning and final action.\n",
            "model\n",
            "<reasoning>\n",
            "This is a tricky spot.  The \"Button\" position is advantageous, but you're starting with a very weak hand, and the pot is relatively small.  We're dealing with a mix of value and potential for bluffing. \n",
            "\n",
            "* **Weak hand:** A-K is pretty marginal, often not strong enough to win in most post-flop situations.\n",
            "* **Early position:**  Button is good but we're also at a low stack, meaning we need to be cautious.\n",
            "* **Pot:**  The pot is small, but it's building quickly due to the blinds, so we want to make sure we're getting value.\n",
            "* **Opponent's actions:** The blinds are both involved, so they are likely to be aggressive post-flop. \n",
            "\n",
            "Here's how I'd break it down:\n",
            "\n",
            "1. **Opponent's range:** Their actions (blind-post) suggest they are likely to bet on the flop with a wide range. \n",
            "2. **My hand:**  A-K is unlikely to improve much in terms of hand strength.\n",
            "3. **Pot size:**  Small pot size makes a bet a high-risk move.\n",
            "4. **Expected value:**   I'm not expecting much from this hand.\n",
            "\n",
            "**Therefore, I would likely fold.**  \n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "**Fold**\n",
            "</answer> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMv9JH90psey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}